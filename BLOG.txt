## Hackathon Blog ##

Day 1

--Our initial thoughts
Is it possible to change the parameters of nanocorrect or nanopolish in order to optimise de-novo assembly

--A plan
Reverse engineer the nanopolish code to find where the statistical parameters are put in. We want to extract these values and put them into a file which we can iteratively change so that we can compute the values for the best possible assembly. Following on - try this on different species

--Difficulties
Code is complex and not straighforward. Nanocorrect may be out of date. We need data. 

Day 2

--Morning re-think
After some more investigation into the mammouth amount of code that makes up nanopolish we had a re-think. New plan is to compare the assembly pipeline when using the two different assembly tools: nanocorrect and canu. Also think how to analyse genome assmebly downstream.

--Got some data!
Thanks to Conrad we got some raw nanopore reads for the budding yeast Schizosaccharomyces pombe! Downloading is proving difficult with the puny amount of space on dtp computers...setting up a AWS server to try and help.

--A new pipeline
with our new plan we have made a new pipeline and identified the programs we need to download/use. Now to get them set up. Harriet has started the presentation!

--A visit from Conrad
Conrad came to see us and gave us some help, although not such good news. We have a few more things to think about. 1)Data size, the yeast reads in total are something like half a terabyte so he only sent us 5gb worth which was already a lot for the DTP computers to handle, good news is though that we have a running AWS server. 2)Latest release of Nanopolish actually takes 1D reads and we're not sure if nanocorrect takes 1D reads, possibly not, plus we need to make sure the version of nanopore the reads were made with matches the version of nanopore which nanopolish/correct is made for. 3)Canu seems to do more than nanocorrect does, we need to work out how nanocorrect would fit into the pipeline. 4) When using canu we need to get information about the size of the genome, the longest reads and things like that. 4)Downstream genome assembly assessment will be strange as the coverage may be so low, will need to think about this

--Current plan
Get at least one pipeline going, so Canu and then nanopolish. Once we get modules in pipeline up and running with the 5GB of reads we could look at feeding through more of the reads (which may take a long time) on AWS. Ben is also planning to write a shell script so that the pipeline can be joined together and run from a single command line!

--Accident
Ctrl^C. Facepalm.

--End of Day 2
As night closes in we say goodbye to our computers, leaving them running with important tasks, and hoping Eoin doesn't remote turn them all off in a fit of rage. The plan has been scaled back somewhat dramatically and now we're taking each step at a time. Let's hope dawn is error message free and a bit warmer. 

--Late night. Still Day 2.
Canu not really working, might be that the small amount of yeast data we can run at a time is simply not enough to form a sequence assembly. Found some alternative raw nanopore data - e.coli - which has a genome ~3x smaller. Still a large amount of data but might be more feasible.

Day 3

--Silly yeast
No luck with the yeast genome data, when run in canu the output file (contigs.fasta) is empty. Probably because coverage is so low that no contigs can be assembled. On the plus side we have got the e-coli data downloaded and going ahead with that.

--Canu, canu, canu
Problems with Canu are occuring. It seems to run with the ecoli data but still the contigs.fasta folder is empty and we can't work out why. The ecoli data is 2D raw nanopore reads. We are only inputting the forward reads to try and simulte 1D data as we need this for nanopolish (I think). The time it takes to run canu and find it doesn't work is makingus think we might need a new plan. Emailed Conrad for help.

--Update
We are trying to canu anything we can to make it work. Ben has started on a side project - investigating a way to amplify low coverage reads artifically by making copies of the fasta files with a mind to then use them to make an assembly, possibly following some manual introduction of mutations. 

--End of day 3
Managed to get some data out of canu using the Normal 2D reads for the ecoli data. Canu assembled 7 contigs, the longest of which is 24000 which is small considering the genome is 4.6MB. Not sure what the problem is there. Cam has started running canu on some new data, the BC01 bacteria, the data for which is already on our machines. 

Day 4

--Deep in the early hours
Ben has gone on a coding bender. We have no idea what he has done, there is a file on github but deciphering the C (or C++??) is another story. It is something to do with the side project mentioned earlier and we await his arrival to be enlightened. 

--Morning 10.30am
Day 4. The pressure heats up. Coffee is a flowing. Snacks abound. Plans today: Cam is still running canu on this new bacteria data, looking more hopeful. I (Rona) am going to run the ecoli data with a different assembler to try and see if the issue is with the data or canu. Harriet is  working hard on the prezi presentation, trying to spin some story out of this. And ben...he has not yet emerged from the night of coding (understandably) but thanks to him we now have a much beefier github repository. 

--Lunchtime
Ran a mapping assembly of the 2D ecoli reads which showed that the reads in the file cover the whole ecoli genome. So the problem must be with the parameters or something in canu. Similar thing (short contig length) has happened with Cam's run of the BC01 data. Grrrr. 
Food needed. 

--OMFG
THE PROBLEM WITH CANU MIGHT OF BEEN A VERSION ISSUE. Ran the BC01 reads on AWS instead and got much better contig lengths. Can't belive this. Re-running the ecoli data now. The suspense is killing me. 

--Fail. Cry.
Did not make any difference running the E.coli data on the AWS. Jumping ship on canu. Something cool: having a look at the ecoli assembly done with a reference on BWA you can clearly see a section with double the amount of coverage. This must correspond with the unexpected insertion of a transposon that was methioned in the Loman paper - cool! Definately shows we got the right data

--Afternoon
Presentation coming on great thanks to Cam and Harriet. Ben is coding away, making a program to simulate increased read coverage. Me? Writing the blog, still thinking about Canu, generally running assembly comparisons and keeping out of google slide battles.



